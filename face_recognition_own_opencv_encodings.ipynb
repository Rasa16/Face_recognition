{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "face_recognition_own_opencv_encodings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1AfMj-jvgB9yHNFsGxvt3nBNO7DS3STeZ",
      "authorship_tag": "ABX9TyN5x1tJdrxD04WOlXDoXQb6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rasa16/Face_recognition/blob/master/face_recognition_own_opencv_encodings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCaxAi8tno8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the necessary packages\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import pickle\n",
        "import cv2\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-g86NScueoT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "8acdbb53-b3bf-49d5-cb6b-95c1c9e004c5"
      },
      "source": [
        "# load our serialized face detector from disk\n",
        "print(\"[INFO] loading face detector...\")\n",
        "protoPath = os.path.sep.join([\"/content/drive/My Drive/Face_recognition_own/face_recognition_own_opencv/opencv-face-recognition/face_detection_model/\", \"deploy.prototxt\"])\n",
        "modelPath = os.path.sep.join([\"/content/drive/My Drive/Face_recognition_own/face_recognition_own_opencv/opencv-face-recognition/face_detection_model/\",\n",
        "\t\"res10_300x300_ssd_iter_140000.caffemodel\"])\n",
        "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
        "\n",
        "# load our serialized face embedding model from disk\n",
        "print(\"[INFO] loading face recognizer...\")\n",
        "embedder = cv2.dnn.readNetFromTorch(\"/content/drive/My Drive/Face_recognition_own/face_recognition_own_opencv/opencv-face-recognition/openface_nn4.small2.v1.t7\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading face detector...\n",
            "[INFO] loading face recognizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOvgQmhIwGIU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a613ccd-c629-4b2b-b686-2b302441cbb7"
      },
      "source": [
        "# grab the paths to the input images in our dataset\n",
        "print(\"[INFO] quantifying faces...\")\n",
        "imagePaths = list(paths.list_images(\"/content/drive/My Drive/Face_recognition_own/My family\"))\n",
        "\n",
        "# initialize our lists of extracted facial embeddings and\n",
        "# corresponding people names\n",
        "knownEmbeddings = []\n",
        "knownNames = []\n",
        "\n",
        "# initialize the total number of faces processed\n",
        "total = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] quantifying faces...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRrij_f3xE7O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "794e414f-d755-48ae-e746-6f32213280c0"
      },
      "source": [
        "# loop over the image paths\n",
        "for (i, imagePath) in enumerate(imagePaths):\n",
        "\t# extract the person name from the image path\n",
        "\tprint(\"[INFO] processing image {}/{}\".format(i + 1,\n",
        "\t\tlen(imagePaths)))\n",
        "\tname = imagePath.split(os.path.sep)[-2]\n",
        "\n",
        "\t# load the image, resize it to have a width of 600 pixels (while\n",
        "\t# maintaining the aspect ratio), and then grab the image\n",
        "\t# dimensions\n",
        "\timage = cv2.imread(imagePath)\n",
        "\timage = imutils.resize(image, width=600)\n",
        "\t(h, w) = image.shape[:2]\n",
        "\n",
        "\t# construct a blob from the image\n",
        "\timageBlob = cv2.dnn.blobFromImage(\n",
        "\t\tcv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
        "\t\t(104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
        "\n",
        "\t# apply OpenCV's deep learning-based face detector to localize\n",
        "\t# faces in the input image\n",
        "\tdetector.setInput(imageBlob)\n",
        "\tdetections = detector.forward()\n",
        " \n",
        "\t# ensure at least one face was found\n",
        "\tif len(detections) > 0:\n",
        "\t\t# we're making the assumption that each image has only ONE\n",
        "\t\t# face, so find the bounding box with the largest probability\n",
        "\t\ti = np.argmax(detections[0, 0, :, 2])\n",
        "\t\tconfidence = detections[0, 0, i, 2]\n",
        "\n",
        "\t\t# ensure that the detection with the largest probability also\n",
        "\t\t# means our minimum probability test (thus helping filter out\n",
        "\t\t# weak detections)\n",
        "\t\tif confidence > 0.5:\n",
        "\t\t\t# compute the (x, y)-coordinates of the bounding box for\n",
        "\t\t\t# the face\n",
        "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "\t\t\t# extract the face ROI and grab the ROI dimensions\n",
        "\t\t\tface = image[startY:endY, startX:endX]\n",
        "\t\t\t(fH, fW) = face.shape[:2]\n",
        "\n",
        "\t\t\t# ensure the face width and height are sufficiently large\n",
        "\t\t\tif fW < 20 or fH < 20:\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t# construct a blob for the face ROI, then pass the blob\n",
        "\t\t\t# through our face embedding model to obtain the 128-d\n",
        "\t\t\t# quantification of the face\n",
        "\t\t\tfaceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
        "\t\t\t\t(96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
        "\t\t\tembedder.setInput(faceBlob)\n",
        "\t\t\tvec = embedder.forward()\n",
        "\n",
        "\t\t\t# add the name of the person + corresponding face\n",
        "\t\t\t# embedding to their respective lists\n",
        "\t\t\tknownNames.append(name)\n",
        "\t\t\tknownEmbeddings.append(vec.flatten())\n",
        "\t\t\ttotal += 1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] processing image 1/136\n",
            "[INFO] processing image 2/136\n",
            "[INFO] processing image 3/136\n",
            "[INFO] processing image 4/136\n",
            "[INFO] processing image 5/136\n",
            "[INFO] processing image 6/136\n",
            "[INFO] processing image 7/136\n",
            "[INFO] processing image 8/136\n",
            "[INFO] processing image 9/136\n",
            "[INFO] processing image 10/136\n",
            "[INFO] processing image 11/136\n",
            "[INFO] processing image 12/136\n",
            "[INFO] processing image 13/136\n",
            "[INFO] processing image 14/136\n",
            "[INFO] processing image 15/136\n",
            "[INFO] processing image 16/136\n",
            "[INFO] processing image 17/136\n",
            "[INFO] processing image 18/136\n",
            "[INFO] processing image 19/136\n",
            "[INFO] processing image 20/136\n",
            "[INFO] processing image 21/136\n",
            "[INFO] processing image 22/136\n",
            "[INFO] processing image 23/136\n",
            "[INFO] processing image 24/136\n",
            "[INFO] processing image 25/136\n",
            "[INFO] processing image 26/136\n",
            "[INFO] processing image 27/136\n",
            "[INFO] processing image 28/136\n",
            "[INFO] processing image 29/136\n",
            "[INFO] processing image 30/136\n",
            "[INFO] processing image 31/136\n",
            "[INFO] processing image 32/136\n",
            "[INFO] processing image 33/136\n",
            "[INFO] processing image 34/136\n",
            "[INFO] processing image 35/136\n",
            "[INFO] processing image 36/136\n",
            "[INFO] processing image 37/136\n",
            "[INFO] processing image 38/136\n",
            "[INFO] processing image 39/136\n",
            "[INFO] processing image 40/136\n",
            "[INFO] processing image 41/136\n",
            "[INFO] processing image 42/136\n",
            "[INFO] processing image 43/136\n",
            "[INFO] processing image 44/136\n",
            "[INFO] processing image 45/136\n",
            "[INFO] processing image 46/136\n",
            "[INFO] processing image 47/136\n",
            "[INFO] processing image 48/136\n",
            "[INFO] processing image 49/136\n",
            "[INFO] processing image 50/136\n",
            "[INFO] processing image 51/136\n",
            "[INFO] processing image 52/136\n",
            "[INFO] processing image 53/136\n",
            "[INFO] processing image 54/136\n",
            "[INFO] processing image 55/136\n",
            "[INFO] processing image 56/136\n",
            "[INFO] processing image 57/136\n",
            "[INFO] processing image 58/136\n",
            "[INFO] processing image 59/136\n",
            "[INFO] processing image 60/136\n",
            "[INFO] processing image 61/136\n",
            "[INFO] processing image 62/136\n",
            "[INFO] processing image 63/136\n",
            "[INFO] processing image 64/136\n",
            "[INFO] processing image 65/136\n",
            "[INFO] processing image 66/136\n",
            "[INFO] processing image 67/136\n",
            "[INFO] processing image 68/136\n",
            "[INFO] processing image 69/136\n",
            "[INFO] processing image 70/136\n",
            "[INFO] processing image 71/136\n",
            "[INFO] processing image 72/136\n",
            "[INFO] processing image 73/136\n",
            "[INFO] processing image 74/136\n",
            "[INFO] processing image 75/136\n",
            "[INFO] processing image 76/136\n",
            "[INFO] processing image 77/136\n",
            "[INFO] processing image 78/136\n",
            "[INFO] processing image 79/136\n",
            "[INFO] processing image 80/136\n",
            "[INFO] processing image 81/136\n",
            "[INFO] processing image 82/136\n",
            "[INFO] processing image 83/136\n",
            "[INFO] processing image 84/136\n",
            "[INFO] processing image 85/136\n",
            "[INFO] processing image 86/136\n",
            "[INFO] processing image 87/136\n",
            "[INFO] processing image 88/136\n",
            "[INFO] processing image 89/136\n",
            "[INFO] processing image 90/136\n",
            "[INFO] processing image 91/136\n",
            "[INFO] processing image 92/136\n",
            "[INFO] processing image 93/136\n",
            "[INFO] processing image 94/136\n",
            "[INFO] processing image 95/136\n",
            "[INFO] processing image 96/136\n",
            "[INFO] processing image 97/136\n",
            "[INFO] processing image 98/136\n",
            "[INFO] processing image 99/136\n",
            "[INFO] processing image 100/136\n",
            "[INFO] processing image 101/136\n",
            "[INFO] processing image 102/136\n",
            "[INFO] processing image 103/136\n",
            "[INFO] processing image 104/136\n",
            "[INFO] processing image 105/136\n",
            "[INFO] processing image 106/136\n",
            "[INFO] processing image 107/136\n",
            "[INFO] processing image 108/136\n",
            "[INFO] processing image 109/136\n",
            "[INFO] processing image 110/136\n",
            "[INFO] processing image 111/136\n",
            "[INFO] processing image 112/136\n",
            "[INFO] processing image 113/136\n",
            "[INFO] processing image 114/136\n",
            "[INFO] processing image 115/136\n",
            "[INFO] processing image 116/136\n",
            "[INFO] processing image 117/136\n",
            "[INFO] processing image 118/136\n",
            "[INFO] processing image 119/136\n",
            "[INFO] processing image 120/136\n",
            "[INFO] processing image 121/136\n",
            "[INFO] processing image 122/136\n",
            "[INFO] processing image 123/136\n",
            "[INFO] processing image 124/136\n",
            "[INFO] processing image 125/136\n",
            "[INFO] processing image 126/136\n",
            "[INFO] processing image 127/136\n",
            "[INFO] processing image 128/136\n",
            "[INFO] processing image 129/136\n",
            "[INFO] processing image 130/136\n",
            "[INFO] processing image 131/136\n",
            "[INFO] processing image 132/136\n",
            "[INFO] processing image 133/136\n",
            "[INFO] processing image 134/136\n",
            "[INFO] processing image 135/136\n",
            "[INFO] processing image 136/136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoVonexLyFpt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d5e1be1-95c9-4e5f-94c1-d79245dec903"
      },
      "source": [
        "# dump the facial embeddings + names to disk\n",
        "print(\"[INFO] serializing {} encodings...\".format(total))\n",
        "data = {\"embeddings\": knownEmbeddings, \"names\": knownNames}\n",
        "f = open(\"/content/drive/My Drive/Face_recognition_own/face_recognition_own_opencv/encodings_own_opencv.pickle\", \"wb\")\n",
        "f.write(pickle.dumps(data))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] serializing 135 encodings...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrMb-hvzzw51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}